{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646267d4-545a-4b9e-9d6a-d497d310ea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Embedding, Dropout, Softmax, Masking\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.callbacks import Callback,ModelCheckpoint,CSVLogger,History\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea96be49-89c0-4f4c-b8b1-eb488902bea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512 # \n",
    "N = 6 # Number of stacks of Encoders and decoders\n",
    "h = 8 # parallel attention layers\n",
    "p_drop = 0.1\n",
    "dff=2048 # first layer of the FFN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248218db-03d7-4069-85d4-b4772fdb28f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(Layer):\n",
    "    def __init__(self,d_model, num_heads, **kwargs):\n",
    "        super(ScaledDotProductAttention, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.scaling_factor = tf.sqrt(d_model/num_heads)\n",
    "\n",
    "   \n",
    "        \n",
    "    def call(self, Q,K,V, causal_mask=None, mask=None): # As tested, mask correspond to the msak of the first arg. Q in this case\n",
    "        \n",
    "    \n",
    "        key_mask = K._keras_mask\n",
    "        out = tf.matmul(Q,K, transpose_b=True) # matmul   \n",
    "        out = tf.divide(out,self.scaling_factor) # scaling\n",
    "\n",
    "       \n",
    "        total_mask = None\n",
    "        # Padding mask will never be none due to embedding layer always passing it  the mask\n",
    "        mask_Q_num = tf.cast(mask, \"float32\")\n",
    "        mask_K_num = tf.cast(key_mask, \"float32\")\n",
    "        mask_QK = tf.matmul(mask_Q_num[...,None],mask_K_num[:,None])\n",
    "        \n",
    "        # Mask with illegal connections due to padding. Here, illegal connections are set True value\n",
    "        illegal_padding_mask = tf.logical_not( tf.cast(mask_QK, dtype=tf.bool) )\n",
    "        total_mask = illegal_padding_mask\n",
    "         \n",
    "    \n",
    "\n",
    "          \n",
    "        if causal_mask is not None: # this is bool mask with illegal connections set to True\n",
    "            #print(\"using causal masking\")\n",
    "            total_mask = tf.logical_or(causal_mask[None],total_mask)\n",
    "\n",
    "        # Setting the illegal connections in the total mask to -infty to make them zero in the softmax computation\n",
    "        out += tf.cast(total_mask, tf.float32) * tf.float32.min\n",
    "\n",
    " \n",
    "        out = tf.nn.softmax(out,axis=-1)\n",
    "        out = out * mask_Q_num[..., None]\n",
    "        \n",
    "        \n",
    "        out = tf.matmul(out,V)\n",
    "        return out\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6885cf9a-2649-442b-ab85-645b44d3c21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Layer):\n",
    "\n",
    "    \"\"\"\n",
    "    Notes: queries, keys, and values will be projected to learned linear projections of \n",
    "    dimension d_k, d_k, d_v\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,d_model, num_heads, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__( **kwargs)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        self.num_heads= num_heads\n",
    "        self.scaledDotProductAttention = ScaledDotProductAttention(d_model, num_heads)\n",
    "        \n",
    "        self.W_Qs = []       # List of Query Weights matrices\n",
    "        self.W_Ks = []       # List of Key Weights matrices\n",
    "        self.W_Vs = []       # List of Values Weights matrices\n",
    "        \n",
    "        d_k = int(d_model/num_heads) # Projected key dimension\n",
    "        d_v = d_k                    # Projected value dimension\n",
    "        \n",
    "        for i in range(num_heads): \n",
    "            self.W_Qs.append(Dense(units = d_k, use_bias=False))\n",
    "            self.W_Ks.append(Dense(units = d_k, use_bias=False))\n",
    "            self.W_Vs.append(Dense(units = d_v, use_bias=False))\n",
    "\n",
    "        self.W_O =  Dense(units = d_model, use_bias=False) \n",
    "\n",
    "    def call(self, Q,K,V, causal_mask=None):\n",
    "        attentionHeads = []\n",
    "        for i in range(self.num_heads):\n",
    "\n",
    "            # Project Querys, Keys and Values\n",
    "            Q_i = self.W_Qs[i](Q) # Queries' Projection for the ith head\n",
    "            K_i = self.W_Ks[i](K) # Keys' Projection for the ith head\n",
    "            V_i = self.W_Vs[i](V) # Values' Projection for the ith head\n",
    "            \n",
    "\n",
    "            attentionHeads.append(self.scaledDotProductAttention(Q_i,K_i,V_i,causal_mask))\n",
    "\n",
    "        # Concatenate all the attention heads on the last (feature) axis\n",
    "        concat_heads = tf.concat(attentionHeads,axis=-1)\n",
    "\n",
    "        # Final linear layer\n",
    "        return self.W_O(concat_heads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1671c01f-983d-41ab-9de3-7c3c5f6ee46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(Layer):\n",
    "    \n",
    "    def __init__(self,dff, **kwargs):\n",
    "        super(FFN, self).__init__()\n",
    "        self.dense1 =  Dense(units=dff, activation='relu')\n",
    "        self.dense2 =  Dense(units = d_model)\n",
    "\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a155133-edd6-445e-9747-03d70a62d3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ResidualBlock, self).__init__(**kwargs)\n",
    "        self.layerNorm = LayerNormalization()\n",
    "        \n",
    "    def call(self, sublayer_in, sub_layer_out, mask=None,):\n",
    "        # masking input before addition. Remember pad values have non-zero vectors\n",
    "        numeric_mask = tf.cast(mask[...,None],tf.float32)\n",
    "        masked_in = sublayer_in*numeric_mask\n",
    "        \n",
    "        x = tf.add(sub_layer_out, masked_in) \n",
    "        x = self.layerNorm(x)\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7456d9f2-3205-45e4-ac27-6b1404a1ca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubLayer1(Layer):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, p_drop, **kwargs):\n",
    "        super(SubLayer1, self).__init__(**kwargs)\n",
    "\n",
    "        self.supports_masking = True # Because SubLayer2 needs masking     \n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.dropout = Dropout(rate=p_drop)\n",
    "        self.resid = ResidualBlock()\n",
    "\n",
    "    def call(self, Q,K,V,X, training, causal_mask=None):\n",
    "        \n",
    "        x = self.mha(Q,K,V,causal_mask)\n",
    "        x = self.dropout(x,training)\n",
    "\n",
    "        x = self.resid(X,x)\n",
    "        return x\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ebe1cb-3580-4095-8e02-545a17bf4cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubLayer2(Layer):\n",
    "    def __init__(self,dff,p_drop, **kwargs):\n",
    "        super(SubLayer2, self).__init__(**kwargs)\n",
    "    \n",
    "        self.ffn = FFN(dff)\n",
    "        self.dropout = Dropout(rate=p_drop)\n",
    "        self.resid = ResidualBlock()\n",
    "        \n",
    "    def call(self, X, training, mask=None ):\n",
    "        \n",
    "        x = self.ffn(X)\n",
    "        x = self.dropout(x, training)\n",
    "\n",
    "        x = self.resid(X,x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07565832-3cc9-4616-8f86-5694872521bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Encoder\n",
    "class EncoderLayer(Layer):\n",
    "\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dff, p_drop, **kwargs):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.supports_masking = True # This ensures the mask it recieves in the call method is passed as is to the next layer.\n",
    "                                    # Maybe another stacked encoder layer or decoder\n",
    "                                    # I confirmed that the the way mask arg of call method is set is if\n",
    "                                    # all args have mask,then mask of the first arg is set.\n",
    "                                    # Setting this variable just attaches mask to the output tensor of the call method.\n",
    "                                    # When the next Layer's call method recieves this masked tensor, the mask arg is set as mentioned.\n",
    "\n",
    "                                    # One thing I want to clarify. This layer doesn't use the mask, so no mask arg needed. However,\n",
    "                                    # The layers called in the call method need masking info. This info is propogated as usual. This is\n",
    "                                    # because encoder_input will have mask, secondly sublayer1 also returns tensor with mask. So each\n",
    "                                    # sublayer gets the mask arg in their call method\n",
    "                        \n",
    "        self.self_attention = SubLayer1(d_model, num_heads, p_drop)\n",
    "        self.position_wise = SubLayer2(dff, p_drop)\n",
    "\n",
    "    def call(self, encoder_input, training, mask=None):\n",
    "        # tokenEmbeddings: Embeddings of tokens in a sentence\n",
    "        #self attention, so Q, K, V are all same\n",
    "        #print(\"training:\",training)\n",
    "        x = encoder_input\n",
    "        x = self.self_attention(x,x,x,x,training)\n",
    "        x = self.position_wise(x,training)\n",
    "        return x\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bd6dea-87e1-404e-9b7d-2b71e19ae22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPositionalEmbedding(seq_length=256, feature_dimension=None):\n",
    "\n",
    "\n",
    "    # Returns featrure_dimension dimensional positional encodings for each position in a sequence of length seq_length.\n",
    "    # return shape (sequ_length, feature_dimension)\n",
    "    dimensions = tf.range(feature_dimension, dtype=tf.float32)\n",
    "    positions = tf.range(seq_length, dtype=tf.float32)[...,None] # add additional dimension at the end for broadcasting\n",
    "    even_dimensions = dimensions[::2]\n",
    "    odd_dimensions   =  dimensions[1::2]\n",
    "\n",
    "    feature_dimension = tf.cast(feature_dimension, tf.float32)\n",
    "    even = tf.sin( positions/tf.pow(10000.,even_dimensions/feature_dimension)  )\n",
    "    odd  = tf.cos( positions/tf.pow(10000.,odd_dimensions/feature_dimension) )\n",
    "\n",
    "    # Since for a given position, concatenating the even and odd dimension is functinonally\n",
    "    # equivalent to interleaving the even and odd dimensions, we will just stack them as follows. (I will later try interleave as well and check the time it takes) \n",
    "\n",
    "    sequence_position_embeddings = tf.concat([even,odd], axis=-1)\n",
    " \n",
    "    return sequence_position_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2468484a-4f6a-4eff-9fa5-d83fde0539df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(Layer):\n",
    "    \"\"\"\n",
    "    This layer takes ouput from the embedding layer. Embedding layer must have output shape of (batch, seq-len, features(d_model))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(PositionalEncoding, self).__init__(**kwargs)\n",
    "        self.supports_masking=True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        # inputs shape should be (batch, seq-len, features(d_model))\n",
    "        #_,seq_len, features = tf.shape(inputs)#[1:]#.numpy()\n",
    "        #_,seq_len, features = tf.split(tf.shape(inputs),3)\n",
    "        shape= tf.shape(inputs)\n",
    "        seq_len, features = shape[1],shape[2]\n",
    "\n",
    "        positionalEncodings = getPositionalEmbedding(seq_len, features)\n",
    "        positionalEncodings = positionalEncodings[None,...] # add batch dimension along which to broadcast\n",
    "\n",
    "        x = tf.add(inputs,positionalEncodings)  # This is equivalent to inputs+positionalEncodings[None]. \n",
    "                                                # Here, I explicitly added a batch dimension for broadcasting, but its not needed.\n",
    "                                                # You can confirm as follows:  print(tf.math.reduce_all((inputs+positionalEncodings[None])==x))\n",
    "        return x\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e145a14-1680-42cf-8a4f-8c38249abb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(Layer):\n",
    "    def __init__(self,d_model, num_heads,dff,p_drop, **kwargs):\n",
    "        super(DecoderLayer, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking=True # Ensures padding mask's propogation to the next decoder layer.\n",
    "        self.masked_self_attention = SubLayer1(d_model, num_heads, p_drop) \n",
    "        self.encoder_decoder_attention = SubLayer1(d_model, num_heads, p_drop)\n",
    "        self.position_wise = SubLayer2(dff, p_drop) # Position-wise feed forward\n",
    "\n",
    "    # Important note: First are should be the tensor whose padding mask needs to be propagated because I chose the easiest way for now.\n",
    "    def call(self,  decoder_input, encoder_output,training, causal_mask):\n",
    "        # tokenEmbeddings: Embeddings of tokens in a sentence\n",
    "        #self attention, so Q, K, V are all same\n",
    "        x = decoder_input\n",
    "        x = self.masked_self_attention(x,x,x,x,training,causal_mask)\n",
    "        x = self.encoder_decoder_attention(x,encoder_output,encoder_output,x, training)\n",
    "        x = self.position_wise(x,training)\n",
    "        return x\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53b7576-9b33-4321-8e02-aed8623a9e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Layer):\n",
    "    def __init__(self,N, d_model, num_heads,dff, p_drop, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "\n",
    "        self.supports_masking=True # Dobule checks that mask gets passed to the next layer. Decoder in this case\n",
    "        self.encoder_layers = []\n",
    "        for n in range(N):\n",
    "            self.encoder_layers.append(EncoderLayer(d_model, num_heads,dff, p_drop))\n",
    "    \n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        x = inputs\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x,training) # Each layer recieves masked tensor because the previous layer has self.supports_mask=True\n",
    "        return x # This output will have mask atthed to encoder_layers's implementation. However, for clarity I can also set the self.supports_masking=True\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3a9661-5b3f-447c-9385-170eec18fd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Layer):\n",
    "    \n",
    "    def __init__(self, N, d_model, num_heads, dff, seq_len, p_drop, **kwargs):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder_layers = []\n",
    "        self.causal_mask = tf.constant(np.triu(np.ones((seq_len,seq_len)),k=1 ), dtype=tf.bool)\n",
    "\n",
    "        for n in range(N):\n",
    "            self.decoder_layers.append(DecoderLayer(d_model, num_heads,dff, p_drop))\n",
    "\n",
    "    def call(self, enc_output,  dec_input, training):\n",
    "        #def call(self, encoder_output, decoder_input, mask):\n",
    "        x = dec_input\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x , enc_output,training,self.causal_mask)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7b90a5-d00c-4bbe-8cf5-d0895f594478",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaling(Layer):\n",
    "    # I created this layer, because simply doing a scalar multiplication with\n",
    "    # a tensor with _keras_mask property resulted in a tensor with no mask.\n",
    "    def __init__(self, scale, **kwargs):\n",
    "        super(Scaling,self).__init__(**kwargs)\n",
    "        self.supports_masking=True\n",
    "        self.scale=scale\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return inputs*self.scale\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6c015c-0901-4626-bb8e-3debf049a1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(Model):\n",
    "    def __init__(self, N, d_model, num_heads,dff,seq_len, vocab_size, p_drop, **kwargs):\n",
    "        super(Transformer,self).__init__(**kwargs)\n",
    "        self.shared_embedding = Embedding(vocab_size+1,d_model,mask_zero=True) # be becasue word indexes staart from 1, but embedding layer's embedding indexes start from 0.0 index is left for padding\n",
    "        self.positional_encoding = PositionalEncoding()\n",
    "        self.encoder = Encoder(N, d_model, num_heads,dff, p_drop)\n",
    "        self.decoder =  Decoder(N, d_model, num_heads,dff,seq_len, p_drop)\n",
    "        self.dropout1 = Dropout(p_drop)\n",
    "        self.dropout2 = Dropout(p_drop)\n",
    "        self.scale_embed =  Scaling(d_model**.5)\n",
    "\n",
    "    def call(self, input, training=False):\n",
    "        \"\"\"\n",
    "        enc_inputs : tokenized sequence of shape (batch, enc_seq_len)\n",
    "        dec_inputs : tokenized sequence of shape (batch, dec_seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        # YYYou missed something very important. Add positional embedings to the input !!!!!!!!!!!\n",
    "        enc_inputs, dec_inputs = input\n",
    "        \n",
    "        enc_inputs = self.shared_embedding(enc_inputs)\n",
    "        enc_inputs = self.scale_embed(enc_inputs)\n",
    "        enc_inputs = self.positional_encoding(enc_inputs)  \n",
    "        enc_inputs = self.dropout1(enc_inputs, training)\n",
    "        \n",
    "        dec_inputs = self.shared_embedding(dec_inputs)\n",
    "        dec_inputs = self.scale_embed(dec_inputs)\n",
    "        dec_inputs = self.positional_encoding(dec_inputs)\n",
    "        dec_inputs = self.dropout2(dec_inputs,training)\n",
    "       \n",
    "        \n",
    "        enc_output = self.encoder(enc_inputs, training)\n",
    "        decoder_output = self.decoder(enc_output , dec_inputs, training) # has shape(batch, dec_input, d_model)\n",
    "\n",
    "        # Final linear operation\n",
    "        embed_weights = self.shared_embedding.weights[0] # has shape (vocab_size,d_model)\n",
    "        embed_weights_scaled = self.scale_embed(embed_weights)\n",
    "\n",
    "        logits = tf.matmul(decoder_output,embed_weights_scaled,transpose_b=True)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bc1366-0384-42df-bdf2-0c66570c9210",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_obj= tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1, reduction='none', from_logits=True)\n",
    "mask_layer = Masking()\n",
    "\n",
    "def loss_func(y_true, y_pred):\n",
    "    \n",
    "    lab_masked = mask_layer(y_true)\n",
    "    mask = tf.cast(lab_masked._keras_mask, tf.float32)\n",
    "    loss = loss_obj(lab_masked[:,:,1:],y_pred[:,:,1:])\n",
    "    masked_loss = loss * mask\n",
    "    \n",
    "    return tf.reduce_sum(masked_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a630ab31-74fa-4b6b-93fb-91eaabdfd14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, warmup_steps=4000, d_model=512):\n",
    "        super(MyLRSchedule,self).__init__()\n",
    "        self.d_model = d_model#tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps=warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        lrate = self.d_model**-.5 * tf.minimum(step**-0.5, step*self.warmup_steps**-1.5)\n",
    "        return lrate\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'warmup_steps':self.warmup_steps,\n",
    "            'd_model':self.d_model,\n",
    "        }\n",
    "        return config\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
